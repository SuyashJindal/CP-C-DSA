𝗼𝗻-𝗦𝘁𝗮𝘁𝗶𝗼𝗻𝗮𝗿𝘆 𝗧𝗶𝗺𝗲 𝗦𝗲𝗿𝗶𝗲𝘀
A non-stationary series has changing patterns over time.

🔹 𝗞𝗲𝘆 𝗳𝗲𝗮𝘁𝘂𝗿𝗲𝘀:
↳ Shocks or structural breaks
↳ Mean & Variance are changing
↳ Presence of trend & Seasonality

🔹 𝗘𝘅𝗮𝗺𝗽𝗹𝗲𝘀:
↳ Stock prices
↳ Monthly revenue
↳ Daily website traffic with weekly cycles

Most of the time series in the business world are non-stationary.

🔄 𝗛𝗼𝘄 𝘁𝗼 𝗰𝗼𝗻𝘃𝗲𝗿𝘁 𝗻𝗼𝗻-𝘀𝘁𝗮𝘁𝗶𝗼𝗻𝗮𝗿𝘆 → 𝘀𝘁𝗮𝘁𝗶𝗼𝗻𝗮𝗿𝘆
✅ 𝗗𝗶𝗳𝗳𝗲𝗿𝗲𝗻𝗰𝗶𝗻𝗴
Subtract the previous value. Removes trend.

✅ 𝗗𝗲𝘀𝗲𝗮𝘀𝗼𝗻𝗮𝗹𝗶𝘇𝗶𝗻𝗴
Estimate the seasonal component and subtract it.

✅ 𝗟𝗼𝗴 / 𝗣𝗼𝘄𝗲𝗿 𝘁𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝘀
Use when variance increases with level.
///////////



 Tree-Based Models (Better Generalization, Faster Prediction)
LightGBM

✅ Very fast & memory-efficient

✅ Handles missing values natively

✅ Excellent for tabular data like options IV

🔧 Tune num_leaves, max_depth, learning_rate, n_estimators

XGBoost

Similar to LightGBM, more robust for small datasets

Performs well with high variance targets like IVs

CatBoost

✅ Categorical handling without encoding

✅ Robust with fewer hyperparameters

Great for real-world noisy data

🧠 Neural Networks (Nonlinear Patterns, Feature Interactions)
MLPRegressor (from sklearn or PyTorch/TensorFlow)

Flexible architecture for non-linear mapping

Useful when you have many engineered features

🔧 Needs proper scaling and regularization (dropout, L2)

TabNet / FT-Transformer (advanced)

Specifically designed for tabular data

Good interpretability + accuracy

More complex but useful if you have lots of strike, delta, or vega-based features

////////////





1. ARIMA (AutoRegressive Integrated Moving Average): Uses a Linear Regression as the base model. Captures autoregressive and moving average terms, along with integrating the differencing of raw observations (to make the time series stationary). Local only (requires loops for each time series). AutoARIMA is my preferred implementation.

2. Prophet: A forecasting tool developed by Facebook. Designed for data with daily observations and seasonal patterns, employing an additive model with yearly, weekly, and daily components. Local forecasting (requires loops).

3. LSTM (Long Short-Term Memory): A type of recurrent neural network (RNN) used in deep learning. It is specifically designed to avoid the long-term dependency problem, making it effective for time series. My preferred implementation is AWS's GluonTS DeepAR. 

4. Holt-Winters Method: A statistical forecasting technique that applies exponential smoothing to capture trend and seasonality in time series data. Useful for data with a clear trend and seasonal pattern. A special case of Exponential Smoothing. 

5. SARIMA (Seasonal ARIMA): An extension of ARIMA that incorporates seasonality. Designed for time series with both non-seasonal and seasonal components, making it more flexible than the standard ARIMA model. AutoARIMA is my preferred implementation.

6. Exponential Smoothing: Applies exponentially decreasing weights to past observations. Best for short-term forecasts, particularly effective for data without trends or seasonal patterns. ETS (Exponential-Trend-Smoothing) is my preferred implementation. 

7. Random Forest: An ensemble learning method that makes many decision trees. The forecast is the mean prediction of the individual trees. Global or local. Scalable. Cannot predict over max/min due to tree-based algorithm problem. 

8. XGBoost: An efficient and scalable implementation of gradient boosting. Known for its high performance and flexibility in handling various types of predictive modeling tasks. Does well at detecting complex seasonality provided feature engineering is performed. Global or local. Cannot predict over global min/max. 
